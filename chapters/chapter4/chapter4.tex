\chapter{Conclusions}\label{chapter4}
This chapter discusses the general conclusions, contributions, 
and the future work.
\section{General Conclusions}

We conclude that there is a widespread issue with security vulnerabilities in container
images used for neuroimaging analyses, and it is likely to impact other
scientific disciplines as well. As shown in our results, it is common for container images
to hold hundreds of vulnerabilities, including several of critical
severity. Container images are impacted regardless of the type of analyses
that they support, and the main OS distributions, such as Ubuntu, Debian, and CentOS
are all affected. This issue can be addressed by the application software
developers by: (1) minifying container images, by using
lightweight OS distributions and reducing software dependencies,
and (2) applying regular security updates, which requires using OS
distributions with long-term support.

Software updates remove about two-thirds of the vulnerabilities found and
should certainly be considered the primary solution to this problem.
However, in neuroimaging as in other disciplines, software updates are
generally discouraged because they can affect analysis results by
introducing numerical perturbations in the
computations~\cite{gronenschild2012effects,glatard2015reproducibility}. We
believe that this position is not viable from an IT security perspective,
and that it could endanger the entire Big Data processing infrastructure,
starting with the HPC centers. Instead, we advocate a more systematic
analysis of the numerical schemes involved in data analyses, which, coupled
with software testing, would make the analyses robust to software updates.
As a first step, the packages impacting the analyses could be specifically
identified and the remaining packages could be updated, which would largely remove
vulnerabilities.

Ultimately, software updates should even occur at runtime rather than when
the container image is built. Indeed, it is likely that container images
used for scientific data analyses be built only occasionally, perhaps every
few weeks when a release becomes available, which may not be compatible
with the frequency of required security updates. In fact, there is no
definite reason for the application software release cycle to be
synchronized with security updates, and security updates shouldn't be
dependent on application software developers. Instead, we believe it would be
relevant for analytics engines to (1) systematically apply security updates
when containers start, and (2) run software tests provided by application
developers, including numerical tests, before running analyses.

Implementing such a workflow, however, requires a long-term endeavour to
evaluate broadly the stability of data analysis pipelines, and to develop
the associated software tests. For the shorter term, we identify the
following recommendations for application developers to reduce the number
of security vulnerabilities in container images:

\begin{enumerate}
\item \emph{Introduce software dependencies cautiously}. Software
dependencies come with a potential security toll that is often neglected.
For instance, it can be tempting to add a complete toolbox to implement a
relatively minor operation in a data analysis pipeline, such as a data
format conversion, while the same functionality might be available in the
existing dependencies of the pipeline, albeit in a less convenient way.
%\item \emph{Use lightweight base images such as Alpine Linux}. Base
%images often come with packages that are useful in personal computers or
%servers, but not in containers dedicated to a specific data analysis. In
%addition, lightweight distributions define packages with a fine
%granularity, allowing developers to avoid installing unnecessary
%dependencies. For instance, the compressed size of the Alpine Linux base
%image on DockerHub is only 2.7~MB while that of the Ubuntu base image is
%27~MB.
\item \emph{Use base image wisely.} Choosing a base image wisely in containers 
helps to reduce security vulnerabilities.
For example, if the scientific data analysis in the image does not require a lot of additional
packages to be installed in the image, choosing a lightweight base image like
Alpine Linux would be a good choice. It is also great to use Alpine as a base image
when all packages required in the image are available in the Alpine repository. 
However, some scientific data analyses
require additional packages to be installed in the image and if they are not
present in the Alpine repository, developers have to build on their own.
Packages that are not installed through the package manager remain unscanned. 
Hence, vulnerabilities in those packages remain
undetected. Therefore, in such cases, base images that have rich
package repository are useful.
\item \emph{Use OS releases with long-term support.} Security updates are
not provided for OS distributions that reached end of life. When a given
release of a data analysis pipeline is expected to be used over a long
period of time, typically several years as it is common in neurosciences,
the life cycle of the distribution release should be considered when
choosing a base container image. OS distributions have very different life
cycle durations, as long and short life cycles serve different purposes.
For instance, among RedHat-based distributions, Fedora release a new version
every 6 months and provide maintenance for about a year, while CentOS
release every 3-5 years and provide maintenance for 10 years. Similarly,
Ubuntu LTS (long-term support) distributions provide free security updates
for 5 years, and Debian stable releases are maintained for 3 years.
% Commercial
% offers for extended maintenance periods are common. 
\item \emph{Install packages, not files}. Vulnerability scanners such as
Anchore, Clair or Vuls detect vulnerabilities from the list of packages
installed in a container image. Therefore, vulnerabilities contained in
software tools installed through direct file download rather than through
the package manager would go completely undetected. Domain-specific
distributions such as \href{http://neuro.debian.net}{NeuroDebian} or
\href{https://docs.fedoraproject.org/en-US/neurofedora/overview/}{NeuroFedora}
in neuroimaging are useful in this respect.
\item \emph{Minify container images.} The automated minification
process that we use in our study is unwieldy for a routine use, as it
requires capturing execution traces with ReproZip to reconstruct the graph
of package dependencies required for the analysis. In practice, it would be
more practical for software developers to identify and remove unnecessary
dependencies when they build containers, based on their knowledge of the
application.
\item \emph{Run image scanners during continuous integration.} Scanning
container images can be a cumbersome process that
could be asynchronously executed during continuous integration (CI),
through tools such as \href{https://travis-ci.org/}{Travis CI} or \href{https://circleci.com/}{CircleCI}. Including security scans in
CI also allows developers to identify vulnerabilities quickly,
before new software versions are released.
\end{enumerate}

\section{Contributions}

All the collected
data in this study are available in our GitHub repository at
\url{https://github.com/big-data-lab-team/container-vulnerabilities-paper}
with a Jupyter notebook to regenerate the figures.
We believe that this
study is the first of its kind to analyze the container images to see
the effect of \textit{image update} and \textit{image minification}.
We provide recommendations on how to build container
images with a reduced amount of vulnerabilities. This work is
submitted and accepted in the GigaScience journal with pending
minor revisions. Also, this work is referenced in the Dark Reading
article at \href{https://www.darkreading.com/application-security/containers-for-data-analysis-are-rife-with-vulnerabilities/d/d-id/1339372}
{https://www.darkreading.com/application-security/containers-for-data-analysis-are-rife-with-vulnerabilities/d/d-id/1339372}

\section{Future Work}

Specific attacks that would exploit vulnerabilities in container images 
against HPC systems should be studied. We
believe that such attacks are likely to exist, although attacking HPC
systems through containers is challenging due to their relative isolation
from the host system. First, under the assumption that legitimate HPC
users can be trusted, attackers would have to be remote to the container,
either in the same network or on a remote network. Two main types of
attacks can be envisaged in these conditions: network-based attacks,
exploiting vulnerabilities in network clients installed in the container,
and data-based attacks, exploiting vulnerabilities through the processing
of malicious data injected through third-party systems.

Several types of escalation attacks could be envisaged once remote
attackers gain access to the container. In particular, attacks related
to (1) using the resources allocated to the container for malicious use resulting in denial of service
for the user running the container and possibly for other HPC users, and
(2) attacking a host network service, for instance a scheduler or a file
system daemon. 
Exploits in the host kernel to break out of the container
are always possible but unlikely, assuming that the host system is
maintained by professional system administrators.

